PPO算法的提出主要是解决在Actor-Critic算法中，由于是通过学习率来更新梯度，如果步子过大就可能导致训练崩溃；如果步子过小可能学习速度很慢。
策略梯度法中，训练数据是通过一次次采样得到的，训练数据与网络参数强相关，可能会导致新策略比就策略还要差。
PPO算法的目的是让训出来的新策略尽量比旧策略好
PPO算法的目标函数就相当于在原有策略梯度目标函数中加入了惩罚项，让模型新策略选择动作概率不要偏离就策略选择相同动作概率太多，这是通过clip项裁剪实现的。
PPO算法的优势函数是采用了广义优势估计（GAE），并没有单纯的计算k步的TD误差，而是计算了k步的TD误差的加权和，这个加权和的权重是k步的TD误差的方差。